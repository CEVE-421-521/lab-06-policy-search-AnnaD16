---
title: "Lab 6: Policy Search"
author: "Your Name (netID)"
jupyter: julia-1.10
date: 2024-03-01
week: 7
categories: [Lab]

format: 
    html: default

    # YOU DO NOT NEED BOTH PDF AND DOCX.
    # COMMENT OR DELETE THE ONE YOU DON'T WANT TO USE.
    # pdf:
    #     documentclass: article
    #     fontsize: 11pt
    #     geometry:
    #         - margin=1in  
    #     number-sections: true
    #     code-line-numbers: true
    docx: 
        toc: true
        fig-format: png
        number-sections: true
        code-line-numbers: true

date-format: "ddd., MMM. D"

execute: 
  cache: true
  freeze: auto

bibliography: references.bib
---
## Set Up
```{julia}
using Revise
using HouseElevation

using CSV
using DataFrames
using DataFramesMeta
using Distributions
using LaTeXStrings
using Metaheuristics
using Plots
using Random
using Unitful

Plots.default(; margin=5Plots.mm)
```

```{julia}
function objective_function(a::AbstractVector{<:AbstractFloat})
    return true # PLACEHOLDER
end
```

## Your Tasks
# Explore
Before digging too deep into the case study, play around with some of the parameters in this optimization tutorial. Vary Dp, the bounds of the optimization problem, the stopping criteria, or the algorithm. Get some intuition and ask any questions you have about the optimization process.

# Optimization
In order to use this optimization package on our problem, we need to define an objective function. This includes not only the objective, but also the SOW(s) over which we will optimize. This also introduces a trade-off: using a few SOWs will make the optimization faster, but may result in a suboptimal solution. Using many SOWs will make the optimization slower, but may result in a more robust solution.

We’ll keep the number of SOWs used for optimization relatively small, and then we’ll evaluate performance (of our “optimal” solution) using a larger number of SOWs.

1. Set your random seed to 2024 so that you always get the same answers when you re-run your code.
```{julia}
Random.seed!(2024)
```

2. Generate N_SOW = 100_000 sows at random as in the previous lab and/or as in the template code provided above.

3. Pick the first N_SOW_opt = 10 of these sows to use for optimization. You can (and should!!) increase this number once you have a working solution, but we’ll use just a few to make sure everything is working.
```{julia}
slr_scenarios = let
    df = CSV.read("data/slr_oddo.csv", DataFrame)
    [Oddo17SLR(a, b, c, tstar, cstar) for (a, b, c, tstar, cstar) in eachrow(df)]
end

house = let
    haz_fl_dept = CSV.read("data/haz_fl_dept.csv", DataFrame) # read in the file
    desc = "Cafeteria Restaurant, structure"
    row = @rsubset(haz_fl_dept, :Description == desc)[1, :] # select the row I want
    area = 672u"ft^2"
    height_above_gauge = 4u"ft"
    House(row; area=area, height_above_gauge=height_above_gauge, value_usd=500_000)
end

p = ModelParams(; house=house, years=2024:2083)

function draw_surge_distribution()
    μ = rand(Normal(20, 15))
    σ = rand(Exponential(1.5))
    ξ = rand(Normal(0.1, 0.05))
    return GeneralizedExtremeValue(μ, σ, ξ)
end
function draw_discount_rate()
    return 0.0
end

N_SOW = 100_000
sows = [
    SOW(rand(slr_scenarios), draw_surge_distribution(), draw_discount_rate()) for
    _ in 1:N_SOW
] # for 10 SOWs

N_SOW_opt = 10
    sows[1:10]
```

4. Define an objective function that takes in a single number as an input (the elevation of the house in feet) and returns one objective function (the net present value of the house for that elevation).
    1. Convert the input scalar to an Action

    2. Call run_sim on each of the N_SOW_opt sows and the elevation to get the expected value of the objective function.

    3. Return the negative of the sum of these expected values (since we are minimizing).
```{julia}
function Object(A)
    npvs = [run_sim(Action(A[1]), sow, p) for sow in sows[1:10]]
    return -mean(npvs)
end
```


5. Test your objective function with a few different elevations to make sure it’s working.
```{julia}
Object(0)
```
```{julia}
Object(5)
```
```{julia}
Object(8)
```
```{julia}
Object(14)
```

6. Run the optimization with the objective function and see what elevation it recommends.
```{julia}
bounds = boxconstraints(; lb=[0], ub=[14]) #NOT COMPLETE

result = optimize(Object, bounds)
```

7. Validate the result by plotting the objective function for a range of elevations (from 0 to 14 ft) using all your SOWs. Is the recommended elevation the minimum? (We’re lucky that in this problem we can compare our optimization solution to a brute-force approach!) If it doesn’t seem to be the minimum:
    1. try increasing N_SOW_opt and see if the result changes.
    2. check whether the optimization algorithm is converging
```{julia}
let
    # Generate a grid of points for the surface plot
    x = range(0; stop=14, length=1000)
    y = Object.(x)

    # Create the surface plot
    plot(
        x, y; xlabel="elevation", ylabel="Object(x)", title=L"Minimize $Object(x)$"
    )
end
```

The recommended elevation (14ft) does correspond to the minimum that appears on the plot above.

# Reflection
1. How are we framing this problem? What are the decision variables, the objective function, and the states of the world over which we optimize?
The decision variables correspond to the different elevations at which the house can be elevated. The objective function takes this as an input to determine the average NPV based on a random ensemble of SOWs and defined parameters describing the properties of the house. Lastly, the SOWs controls for different sea level rise scenarios, possible storm surge distributions, and discount rates.

2. Digging deeper, we are averaging the objective function computed over a finite number of states of the world. This assumes that they are all drawn from a distribution representing the “true” distribution of states of the world. Is this a good assumption?
It is not a good idea to assume this will give us the "true" distribution of SOWs, while it can get us very close there are many uncertainties that cannot be accounted for which will get in the way of this.  

3. What’s not being considered in this analysis that might be important? One thing that is not being considered here is that the value of the house could fluctuate, for example if it gets rebuilt or if significant changes are made to the neighborhood the fixed value that was attributed to the property. It is also difficult to predict the frequency and severness of storm surges which can accumulate to sever damages ultimately impacting the NPV.